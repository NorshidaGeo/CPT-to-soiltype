{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified data loading (supervised)\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"..\") / \"data\" / \"model_ready\"\n",
    "CSV = DATA_DIR / \"cpt_dataset_total.csv\"\n",
    "\n",
    "# Features and label\n",
    "FEATURES = [\n",
    "    \"Depth (m)\",\n",
    "    \"qc (MPa)\",\n",
    "    \"fs (kPa)\",\n",
    "    \"Rf (%)\",\n",
    "    \"σ,v (kPa)\",\n",
    "    \"u0 (kPa)\",\n",
    "    \"σ',v (kPa)\",\n",
    "    \"Qtn (-)\",\n",
    "    \"Fr (%)\",\n",
    "]\n",
    "LABEL = \"Oberhollenzer_classes\"\n",
    "\n",
    "# Load dataset and split into features/label\n",
    "df = pd.read_csv(CSV)\n",
    "X = df[FEATURES].copy()\n",
    "y = df[LABEL].copy()\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"X: {X.shape} | y: {y.shape}\")\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76094a5c",
   "metadata": {},
   "source": [
    "# Hydra Configuration System: A Hands-on Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Hydra for clean, reproducible configuration management in applied machine learning. We'll use a synthetic geotechnical-like dataset and show two patterns:\n",
    "\n",
    "- Programmatic API: initialize/compose configs directly in cells (great inside notebooks).\n",
    "- Decorator API: `@hydra.main` for scripts (shown inline but executed carefully in notebooks).\n",
    "\n",
    "What you'll learn:\n",
    "- Organizing configs with groups (dataset/, model/) and defaults\n",
    "- Overriding values at runtime\n",
    "- Saving outputs to per-run directories\n",
    "- Running simple parameter sweeps in-notebook (no plugins needed)\n",
    "- Logging a run to MLflow while keeping Hydra as the source of truth\n",
    "\n",
    "We keep most code at the top-level in cells for easier inspection and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6639c",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We rely on packages already declared in `pyproject.toml` (hydra-core, numpy, pandas, scikit-learn, seaborn, matplotlib, mlflow). If your environment isn't synced, run `uv sync` before executing the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54178b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moved __future__ import to the first cell\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hydra import compose, initialize_config_dir\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "# Paths for configs and outputs\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "CONFIG_DIR = PROJECT_ROOT / \"notebooks\" / \"config\" / \"hydra_demo\"\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"notebooks\" / \"outputs\" / \"hydra\"\n",
    "CONFIG_DIR.exists(), OUTPUT_ROOT.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9883bcb",
   "metadata": {},
   "source": [
    "## Inspect the Config Tree\n",
    "\n",
    "We created a tiny config tree in `notebooks/config/hydra_demo/`:\n",
    "- `main.yaml` with defaults and Hydra's run dir\n",
    "- `dataset/synthetic.yaml` with dataset parameters\n",
    "- `model/logreg.yaml` with model hyperparameters\n",
    "\n",
    "Let's load and print the composed config using Hydra's programmatic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778b218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize_config_dir(version_base=\"1.3\", config_dir=str(CONFIG_DIR)):\n",
    "    cfg = compose(config_name=\"main.yaml\")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034093a",
   "metadata": {},
   "source": [
    "## Generate a Synthetic CPT-like Dataset\n",
    "\n",
    "We'll simulate a small dataset with features often seen in CPT contexts (depth, qc, fs, Fr, noise) and a 3-class target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c166bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset settings from cfg.dataset\n",
    "n = int(cfg.dataset.size)\n",
    "test_size = float(cfg.dataset.test_size)\n",
    "features = list(cfg.dataset.features)\n",
    "\n",
    "# Synthetic generation\n",
    "depth = np.sort(np.random.uniform(0.0, 30.0, size=n))\n",
    "qc = np.clip(np.random.lognormal(mean=1.4, sigma=0.6, size=n), 0.2, None)\n",
    "fs = np.clip(np.random.lognormal(mean=-0.2, sigma=0.5, size=n) * 0.1, 0.002, None)\n",
    "Fr = 100.0 * fs / np.clip(qc, 1e-6, None)\n",
    "noise = np.random.normal(0.0, 1.0, size=n)\n",
    "\n",
    "# Simple label rule for demo (3 classes)\n",
    "y = np.where((qc > 7.5) & (Fr < 1.5), 0, np.where((qc < 2.5) & (Fr > 2.5), 2, 1))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"depth\": depth, \"qc\": qc, \"fs\": fs, \"Fr\": Fr, \"noise\": noise, \"y\": y}\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774681ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    df[\"y\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .plot(kind=\"bar\", color=[\"#4C72B0\", \"#55A868\", \"#C44E52\"])\n",
    ")\n",
    "ax.set_xlabel(\"class\")\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_title(\"Class balance (synthetic)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488534aa",
   "metadata": {},
   "source": [
    "## Single Run with Programmatic Hydra Compose\n",
    "\n",
    "We'll read model config, fit Logistic Regression with scaling, evaluate accuracy, and save artifacts to the Hydra run directory specified in `main.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features].copy()\n",
    "y = df[\"y\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=cfg.experiment.seed, stratify=y\n",
    ")\n",
    "\n",
    "# Model hyperparameters from cfg.model\n",
    "C = float(cfg.model.C)\n",
    "penalty = str(cfg.model.penalty)\n",
    "max_iter = int(cfg.model.max_iter)\n",
    "solver = str(cfg.model.solver)\n",
    "\n",
    "# Optional scaling\n",
    "if str(cfg.model.scaler) == \"standard\":\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "else:\n",
    "    X_train_s, X_test_s = X_train.values, X_test.values\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    C=C, penalty=penalty, max_iter=max_iter, solver=solver, n_jobs=None\n",
    ")\n",
    "clf.fit(X_train_s, y_train)\n",
    "y_pred = clf.predict(X_test_s)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# Create run directory based on hydra.run.dir\n",
    "with initialize_config_dir(version_base=\"1.3\", config_dir=str(CONFIG_DIR)):\n",
    "    run_cfg = compose(config_name=\"main.yaml\")\n",
    "run_dir = Path(OmegaConf.to_container(run_cfg.hydra.run.dir, resolve=True))\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save artifacts\n",
    "(run_dir / \"config.yaml\").write_text(OmegaConf.to_yaml(cfg))\n",
    "df.head(50).to_csv(run_dir / \"preview.csv\", index=False)\n",
    "pd.DataFrame({\"feature\": features}).to_csv(run_dir / \"features.csv\", index=False)\n",
    "print(\"Artifacts saved to:\", run_dir)\n",
    "run_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26b73f",
   "metadata": {},
   "source": [
    "## Overrides: Change Config Values on the Fly\n",
    "\n",
    "Hydra allows overriding config values without editing YAML. Below we override `model.C` and `dataset.test_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize_config_dir(version_base=\"1.3\", config_dir=str(CONFIG_DIR)):\n",
    "    cfg_over = compose(\n",
    "        config_name=\"main.yaml\", overrides=[\"model.C=10.0\", \"dataset.test_size=0.3\"]\n",
    "    )\n",
    "print(OmegaConf.to_yaml(cfg_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d2730",
   "metadata": {},
   "source": [
    "## Simple Parameter Sweep (in-notebook)\n",
    "\n",
    "We'll emulate a small grid search using config values. This isn't using the Hydra multirun CLI, but it demonstrates how configs drive experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_C = list(cfg.model.grid.C)\n",
    "penalty_values = list(cfg.model.grid.penalty)\n",
    "results = []\n",
    "for C_val, pen in product(grid_C, penalty_values):\n",
    "    with initialize_config_dir(version_base=\"1.3\", config_dir=str(CONFIG_DIR)):\n",
    "        cfg_try = compose(\n",
    "            config_name=\"main.yaml\",\n",
    "            overrides=[f\"model.C={C_val}\", f\"model.penalty={pen}\"],\n",
    "        )\n",
    "    # Prepare data\n",
    "    X = df[features].copy()\n",
    "    y = df[\"y\"].copy()\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=float(cfg_try.dataset.test_size),\n",
    "        random_state=cfg_try.experiment.seed,\n",
    "        stratify=y,\n",
    "    )\n",
    "    if str(cfg_try.model.scaler) == \"standard\":\n",
    "        sc = StandardScaler()\n",
    "        X_tr_s = sc.fit_transform(X_tr)\n",
    "        X_te_s = sc.transform(X_te)\n",
    "    else:\n",
    "        X_tr_s, X_te_s = X_tr.values, X_te.values\n",
    "    model = LogisticRegression(\n",
    "        C=float(cfg_try.model.C),\n",
    "        penalty=str(cfg_try.model.penalty),\n",
    "        max_iter=int(cfg_try.model.max_iter),\n",
    "        solver=str(cfg_try.model.solver),\n",
    "    )\n",
    "    model.fit(X_tr_s, y_tr)\n",
    "    acc_te = accuracy_score(y_te, model.predict(X_te_s))\n",
    "    results.append({\"C\": float(C_val), \"penalty\": str(pen), \"acc\": acc_te})\n",
    "res_df = pd.DataFrame(results).sort_values(\"acc\", ascending=False)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11ff13",
   "metadata": {},
   "source": [
    "## Logging One Run with MLflow\n",
    "\n",
    "We'll log parameters and metrics to MLflow to show how Hydra configs can drive experiment tracking. The MLflow tracking directory for this repo is `experiments/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(str((PROJECT_ROOT / \"experiments\").resolve()))\n",
    "mlflow.set_experiment(\"hydra_demo_notebook\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"single_run\"):\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"dataset\": cfg.dataset.name,\n",
    "            \"test_size\": float(cfg.dataset.test_size),\n",
    "            \"features\": \",\".join(features),\n",
    "            \"model\": cfg.model.name,\n",
    "            \"C\": float(cfg.model.C),\n",
    "            \"penalty\": str(cfg.model.penalty),\n",
    "            \"solver\": str(cfg.model.solver),\n",
    "            \"max_iter\": int(cfg.model.max_iter),\n",
    "        }\n",
    "    )\n",
    "    mlflow.log_metric(\"accuracy\", float(acc))\n",
    "    mlflow.log_artifact(str(run_dir / \"config.yaml\"))\n",
    "    mlflow.log_artifact(str(run_dir / \"preview.csv\"))\n",
    "\n",
    "print(\n",
    "    \"Logged to MLflow experiment hydra_demo_notebook. To view: cd experiments && mlflow ui\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939fbb88",
   "metadata": {},
   "source": [
    "## Using the `@hydra.main` Decorator (script-style)\n",
    "\n",
    "In normal Python scripts, you'd use `@hydra.main(config_path, config_name)`. In notebooks, this is trickier because Hydra changes working directories and manages singleton state. Below is an illustrative snippet; it's commented to avoid conflicts when re-running cells. Copy into a `.py` script to try outside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce667e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hydra import main\n",
    "# from omegaconf import DictConfig\n",
    "#\n",
    "# @main(version_base='1.3', config_path='notebooks/config/hydra_demo', config_name='main.yaml')\n",
    "# def app(cfg: DictConfig):\n",
    "#     print(OmegaConf.to_yaml(cfg))\n",
    "#     # Your training/eval code here using cfg.*\n",
    "#\n",
    "# if __name__ == '__main__':\n",
    "#     app()\n",
    "\n",
    "print(\"Decorator example provided as a reference for scripts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfedb3bf",
   "metadata": {},
   "source": [
    "## Student Task\n",
    "\n",
    "- Add a new parameter to `model/logreg.yaml` (e.g., change `solver` to `saga` and add `penalty: [l1, l2]` in the grid).\n",
    "- In the sweep cell, include the new parameter in the grid and re-run to compare results.\n",
    "- Override the dataset test split (e.g., `dataset.test_size=0.2`) via compose overrides and observe the effect on accuracy.\n",
    "- Bonus: Add a second dataset config (e.g., `dataset/small.yaml`) with a smaller `size` and switch to it via defaults override."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
